{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4de4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import mne\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from lameg.invert import coregister, invert_ebb, load_source_time_series\n",
    "from lameg.laminar import sliding_window_model_comparison\n",
    "from lameg.util import get_fiducial_coords, get_surface_names, load_meg_sensor_data, get_bigbrain_layer_boundaries\n",
    "import spm_standalone\n",
    "import matlab\n",
    "import h5py\n",
    "from mne import read_epochs\n",
    "from scipy.spatial import cKDTree, KDTree\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def get_fiducial_coords(SUB, file_path):\n",
    "    \"\"\"\n",
    "    Get the fidicials from each participants in the specified file \n",
    "    coordinates are reconstructed and with FS RAS offset\n",
    "    \"\"\"\n",
    "\n",
    "    for file in os.listdir(file_path):\n",
    "        if file.endswith(\".tsv\"):\n",
    "            full_file_path = os.path.join(file_path,file)\n",
    "            df = pd.read_csv(full_file_path, sep='\\t')\n",
    "            print(\"Found tsv file for fiducials\")\n",
    "\n",
    "            data = df[df[\"Subject ID\"] == SUB]\n",
    "            nas = data.iloc[:, 5].values\n",
    "            nas = [float(item) for item in nas[0].split(', ')]\n",
    "\n",
    "            lpa = data.iloc[:, 6].values \n",
    "            lpa = [float(item) for item in lpa[0].split(', ')]\n",
    "\n",
    "            rpa = data.iloc[:, 7].values\n",
    "            rpa = [float(item) for item in rpa[0].split(', ')]\n",
    "\n",
    "            return nas, lpa, rpa\n",
    "        \n",
    "def upload_meg_file(rawdata_path, sub, ses, condition):\n",
    "    \"\"\"\n",
    "    Search and upload the raw datachunck\n",
    "\n",
    "    :param rawdata_path: datapath\n",
    "    :param sub: subjects id\n",
    "    :param ses: session id \n",
    "    :param condition: experimental condition (block-wise)\n",
    "    \"\"\"\n",
    "    \n",
    "    sub_meg_path = os.path.join(rawdata_path, \"sub-\" + sub , \"ses-0\" + ses , \"meg/sub-\"+ sub + \"_ses-0\" + ses + \"_task-\" + condition + \"_acq-\")\n",
    "    folders_acq = glob.glob(sub_meg_path + \"*_meg.ds\", recursive=True)\n",
    "    list_folders = sorted(folders_acq, key=lambda x: int(x.split('acq-')[1].split('_')[0]))\n",
    "    \n",
    "    return list_folders\n",
    "\n",
    "def get_roi_idx(subj_id, surf_dir, hemi, regions, surf):\n",
    "    \"\"\"\n",
    "    Returns the vertex indices from a downsampled surface mesh that correspond\n",
    "    to specified anatomical regions (from FreeSurfer annotations).\n",
    "\n",
    "    Parameters:\n",
    "    - subj_id (str): FreeSurfer subject ID.\n",
    "    - surf_dir (str): Path to directory containing the postprocessed surfaces (e.g., pial.gii).\n",
    "    - hemi (str or None): Hemisphere ('lh', 'rh', or None for both).\n",
    "    - regions (list of str): List of region names to extract from the annotation.\n",
    "    - surf (nib.GiftiImage): Downsampled surface mesh used for laminar inference.\n",
    "\n",
    "    Returns:\n",
    "    - roi_idx (np.ndarray): Array of indices into `surf` corresponding to the selected ROI.\n",
    "    \"\"\"\n",
    "    fs_subjects_dir = os.getenv('SUBJECTS_DIR')\n",
    "    fs_subject_dir = os.path.join(fs_subjects_dir, subj_id)\n",
    "\n",
    "    roi_idx = []\n",
    "    hemis = []\n",
    "    if hemi is None:\n",
    "        hemis.extend(['lh', 'rh'])\n",
    "    else:\n",
    "        hemis.append(hemi)\n",
    "    for hemi in hemis:\n",
    "        pial = nib.load(os.path.join(surf_dir, f'{hemi}.pial.gii'))\n",
    "\n",
    "        #         annotation = os.path.join(fs_subject_dir, 'label', f'{hemi}.aparc.annot')\n",
    "        annotation = os.path.join(fs_subject_dir, 'label', f'{hemi}.BA_exvivo.annot')\n",
    "        label, ctab, names = nib.freesurfer.read_annot(annotation)\n",
    "\n",
    "        name_indices = [names.index(region.encode()) for region in regions]\n",
    "        orig_vts = np.where(np.isin(label, name_indices))[0]\n",
    "\n",
    "        # Find the original vertices closest to the downsampled vertices\n",
    "        kdtree = KDTree(pial.darrays[0].data[orig_vts, :])\n",
    "        # Calculate the percentage of vertices retained\n",
    "        dist, vert_idx = kdtree.query(surf.darrays[0].data, k=1)\n",
    "        hemi_roi_idx = np.where(dist == 0)[0]\n",
    "        roi_idx = np.union1d(roi_idx, hemi_roi_idx)\n",
    "    return roi_idx.astype(int)\n",
    "\n",
    "\n",
    "def get_cortical_thickness(multilayer_mesh, n_layers):\n",
    "    \"\"\"\n",
    "    Estimates cortical thickness at each vertex from a multilayer surface mesh.\n",
    "\n",
    "    Parameters:\n",
    "    - multilayer_mesh (nib.GiftiImage): Combined mesh of all depth layers (equidistant).\n",
    "    - n_layers (int): Total number of cortical depth surfaces in the mesh.\n",
    "\n",
    "    Returns:\n",
    "    - thickness (np.ndarray): Vertex-wise Euclidean distance between layer 1 and layer N.\n",
    "    \"\"\"\n",
    "    verts_per_surf = int(multilayer_mesh.darrays[0].data.shape[0] / n_layers)\n",
    "    thickness = np.sqrt(np.sum((multilayer_mesh.darrays[0].data[:verts_per_surf, :] - multilayer_mesh.darrays[0].data[\n",
    "                                                                                      (n_layers - 1) * verts_per_surf:,\n",
    "                                                                                      :]) ** 2, axis=-1))\n",
    "    return thickness\n",
    "\n",
    "\n",
    "def get_lead_field_rmse(gainmat_fname, n_layers, verts_per_surf):\n",
    "    \"\"\"\n",
    "    Computes the RMSE of lead field vectors across cortical depths\n",
    "    relative to the superficial (layer 1) model, for each vertex.\n",
    "\n",
    "    Parameters:\n",
    "    - gainmat_fname (str): Path to HDF5 file containing the gain matrix ('G').\n",
    "    - n_layers (int): Number of cortical depth layers.\n",
    "    - verts_per_surf (int): Number of vertices per layer.\n",
    "\n",
    "    Returns:\n",
    "    - rmse (np.ndarray): RMSE between deep and superficial lead fields per vertex.\n",
    "    \"\"\"\n",
    "    with h5py.File(gainmat_fname, 'r') as file:\n",
    "        lf_mat = file['G'][()]\n",
    "\n",
    "    layer_lf_mat = np.zeros((verts_per_surf, n_layers, lf_mat.shape[1]))\n",
    "    diff_layer_lf_mat = np.zeros((verts_per_surf, n_layers))\n",
    "    for i in range(n_layers):\n",
    "        layer_lf_mat[:, i, :] = lf_mat[i * verts_per_surf:(i + 1) * verts_per_surf, :]\n",
    "    for j in range(verts_per_surf):\n",
    "        for i in range(n_layers):\n",
    "            diff_layer_lf_mat[j, i] = np.sqrt(np.mean((layer_lf_mat[j, i, :] - layer_lf_mat[j, 0, :]) ** 2))\n",
    "    return diff_layer_lf_mat[:, -1]\n",
    "\n",
    "\n",
    "def get_orientation(scalp_mesh_fname, multilayer_mesh, pial_ds_mesh, verts_per_surf):\n",
    "    \"\"\"\n",
    "    Computes the alignment between cortical column orientation vectors and\n",
    "    local scalp surface normals for each vertex.\n",
    "\n",
    "    Parameters:\n",
    "    - scalp_mesh_fname (str): Path to scalp surface (GIFTI).\n",
    "    - multilayer_mesh (nib.GiftiImage): Mesh with orientation vectors per vertex.\n",
    "    - pial_ds_mesh (nib.GiftiImage): Downsampled pial mesh for mapping.\n",
    "    - verts_per_surf (int): Number of vertices per cortical surface.\n",
    "\n",
    "    Returns:\n",
    "    - dot_products (np.ndarray): Cosine similarity (abs dot product) between\n",
    "                                 dipole and scalp normals per vertex.\n",
    "\n",
    "    Notes:\n",
    "    - Higher values indicate more radial orientations\n",
    "    \"\"\"\n",
    "    scalp_mesh = nib.load(scalp_mesh_fname)\n",
    "    scalp_vertices = scalp_mesh.darrays[1].data  # Vertex coordinates\n",
    "    scalp_faces = scalp_mesh.darrays[0].data  # Face indices\n",
    "\n",
    "    pial_ds_vertices = pial_ds_mesh.darrays[0].data  # Vertex coordinates\n",
    "    multilayer_orientations = multilayer_mesh.darrays[2].data  # Orientation vectors\n",
    "\n",
    "    # Compute surface normals\n",
    "    def compute_normals(vertices, faces):\n",
    "        normals = np.zeros_like(vertices)\n",
    "        for i in range(faces.shape[0]):\n",
    "            v0, v1, v2 = vertices[faces[i]]\n",
    "            # Edge vectors\n",
    "            edge1 = v1 - v0\n",
    "            edge2 = v2 - v0\n",
    "            # Cross product to get normal\n",
    "            normal = np.cross(edge1, edge2)\n",
    "            normal /= np.linalg.norm(normal)  # Normalize the normal\n",
    "            normals[faces[i]] += normal\n",
    "        normals /= np.linalg.norm(normals, axis=1)[:, np.newaxis]  # Normalize all normals\n",
    "        return normals\n",
    "\n",
    "    normals = compute_normals(scalp_vertices, scalp_faces)\n",
    "\n",
    "    # Build a KD-tree for the scalp vertices\n",
    "    tree = cKDTree(scalp_vertices)\n",
    "\n",
    "    # Find the nearest neighbor in the scalp mesh for each vertex in the downsampled cortical mesh\n",
    "    _, indices = tree.query(pial_ds_vertices)\n",
    "\n",
    "    # Compute the dot product between the orientation and the normal vectors\n",
    "    dot_products = np.abs(np.einsum('ij,ij->i', normals[indices], multilayer_orientations[:verts_per_surf, :]))\n",
    "    return dot_products\n",
    "\n",
    "\n",
    "def get_dist_to_scalp(scalp_mesh_fname, pial_ds_mesh):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance from each vertex on the cortical surface\n",
    "    to the closest point on the scalp.\n",
    "\n",
    "    Parameters:\n",
    "    - scalp_mesh_fname (str): Path to scalp surface (GIFTI).\n",
    "    - pial_ds_mesh (nib.GiftiImage): Downsampled pial surface.\n",
    "\n",
    "    Returns:\n",
    "    - distances (np.ndarray): Distance (in mm) from cortex to nearest scalp point per vertex.\n",
    "    \"\"\"\n",
    "    scalp_mesh = nib.load(scalp_mesh_fname)\n",
    "    scalp_vertices = scalp_mesh.darrays[1].data  # Vertex coordinates\n",
    "\n",
    "    pial_ds_vertices = pial_ds_mesh.darrays[0].data  # Vertex coordinates\n",
    "\n",
    "    # Build a KD-tree for the scalp vertices\n",
    "    tree = cKDTree(scalp_vertices)\n",
    "\n",
    "    # Find the nearest neighbor in the scalp mesh for each vertex in the downsampled cortical mesh\n",
    "    distances, _ = tree.query(pial_ds_vertices)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def convert_mgz_to_nii(mri_fname, mri_out_path):\n",
    "    \"\"\"\n",
    "    Convert mgz files to .nii - the orig.mgz fpt the mutilayer surface and the T1.mgz for the SPM segmentation\n",
    "    \"\"\"\n",
    "    # function to convert .mgz and to nii file for coregistration\n",
    "\n",
    "    if mri_fname.endswith(\".mgz\"):\n",
    "        nii_fname_path = os.path.join(mri_out_path, os.path.basename(mri_fname)[:-4] + \".nii\")\n",
    "        if not os.path.exists(nii_fname_path):\n",
    "            print(\"The .nii file does not exist. Conversion.\")\n",
    "            img = nib.load(mri_fname)\n",
    "            nib.save(img, nii_fname_path)\n",
    "            print(f\"Conversion finished find .nii file {nii_fname_path}\")\n",
    "        else:\n",
    "            print(f\"File .nii already exists at {nii_fname_path}. Skipping conversion.\")\n",
    "        return nii_fname_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meg_rawdata_path = \"\"\n",
    "braindyn_files = \"\"\n",
    "\n",
    "\n",
    "SUB = ['105', '107', '109', '113', '114',\n",
    "           '115', '117','118','119', '123', \n",
    "           '124', '126','127','129', '130',\n",
    "           '131','132','133','134','135', \n",
    "           '136', '137','138', '141','142',\n",
    "           '143','144','145']\n",
    "ses = \"1\"\n",
    "condition = \"V100V100\"\n",
    "mxf_option = \"no_mxf\" \n",
    "triggers = [\"left_stim\", \"right_stim\"] \n",
    "n_layers = 11\n",
    "\n",
    "base_out_dir = os.path.join('')\n",
    "\n",
    "def run(spm):\n",
    "    for s, sub in enumerate(SUB): \n",
    "        print(f\"Now processing part {sub}\")\n",
    "        session = 'ses-01'\n",
    "        subject = 'sub-' + sub\n",
    "        out_dir = os.path.join(base_out_dir, subject, session)\n",
    "        if not os.path.exists(os.path.join(base_out_dir, subject)):\n",
    "            os.mkdir(os.path.join(base_out_dir, subject))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "\n",
    "        mri_data_path = braindyn_files\n",
    "        mri_out_path = os.path.join(mri_data_path, f\"{sub}-synth\", \"mri\")\n",
    "        mri_fname_orig = os.path.join(mri_data_path, f\"{sub}-synth\", \"mri\", \"T1.mgz\")\n",
    "        mri_fname = convert_mgz_to_nii(mri_fname_orig, mri_out_path)\n",
    "\n",
    "        subj_surf_dir = os.path.join(mri_data_path, f'{sub}-synth', 'layer_surf')\n",
    "        multilayer_mesh_fname = os.path.join(subj_surf_dir, f'sub-{sub}-multilayer.11.ds.link_vector.fixed.gii')\n",
    "        multilayer_mesh = nib.load(multilayer_mesh_fname)\n",
    "        verts_per_surf = int(multilayer_mesh.darrays[0].data.shape[0] / n_layers)\n",
    "        ds_pial = nib.load(os.path.join(subj_surf_dir, 'pial.ds.gii'))\n",
    "        layer_fnames = get_surface_names(\n",
    "                n_layers,\n",
    "                subj_surf_dir,\n",
    "                'link_vector.fixed')\n",
    "\n",
    "\n",
    "        nas, lpa, rpa = get_fiducial_coords(sub, braindyn_files)\n",
    "\n",
    "        print(nas)\n",
    "        print(lpa)\n",
    "        print(rpa)\n",
    "\n",
    "        for trigger in triggers:\n",
    "            out_fname = os.path.join(base_out_dir, f'results_{subject}_{session}_{trigger}.npz')\n",
    "\n",
    "            orig_data_file = f\"mspm_sub-{sub}_ses-0{ses}_task-{condition}_meg_{mxf_option}_{trigger}_nrg_ERF_epo.mat\"\n",
    "            list_folders = upload_meg_file(meg_rawdata_path, sub, ses, condition)\n",
    "            mxf_path_dir = os.path.join(list_folders[-1], f\"mxf_{mxf_option}_data\") \n",
    "            data_dir = os.path.join(mxf_path_dir, \"mxf_sess_data\", \"spm_convert_epo\")\n",
    "\n",
    "            base_ERF_fname = os.path.join(data_dir,orig_data_file)\n",
    "            data_base = os.path.splitext(orig_data_file)[0]\n",
    "\n",
    "                # Copy data files to tmp directory if we need simulations\n",
    "            shutil.copy(\n",
    "                    os.path.join(data_dir, f'{data_base}.mat'),\n",
    "                    os.path.join(out_dir, f'{data_base}.mat')\n",
    "                )\n",
    "            shutil.copy(\n",
    "                    os.path.join(data_dir, f'{data_base}.dat'),\n",
    "                    os.path.join(out_dir, f'{data_base}.dat')\n",
    "                )\n",
    "\n",
    "            base_fname = os.path.join(out_dir, f'{data_base}.mat')\n",
    "\n",
    "                # Coregister data to multilayer mesh\n",
    "            coregister(\n",
    "                    nas,\n",
    "                    lpa,\n",
    "                    rpa,\n",
    "                    mri_fname,\n",
    "                    multilayer_mesh_fname,\n",
    "                    base_fname,\n",
    "                    spm_instance=spm,\n",
    "                    viz=False\n",
    "                )\n",
    "\n",
    "            patch_size = 5\n",
    "                # Run localizer on multilayer mesh on 50 to 150ms time window\n",
    "            [_, _, MU] = invert_ebb(\n",
    "                    multilayer_mesh_fname,\n",
    "                    base_fname,\n",
    "                    n_layers,\n",
    "                    woi=[50, 150],\n",
    "                    patch_size=patch_size,\n",
    "                    n_temp_modes=4,\n",
    "                    n_spatial_modes=60,\n",
    "                    # n_spatial_modes=rank['mag'],\n",
    "                    return_mu_matrix=True,\n",
    "                    spm_instance=spm,\n",
    "                    viz=False\n",
    "                )\n",
    "\n",
    "            lh_roi_idx = get_roi_idx(f\"{sub}-synth\", subj_surf_dir, 'lh', ['V1_exvivo'], ds_pial)\n",
    "            rh_roi_idx = get_roi_idx(f\"{sub}-synth\", subj_surf_dir, 'rh', ['V1_exvivo'], ds_pial)\n",
    "\n",
    "\n",
    "            all_layer_ts, ts_time, _ = load_source_time_series(\n",
    "                    base_fname,\n",
    "                    mu_matrix=MU,\n",
    "                    vertices=(5 - 1) * verts_per_surf + np.arange(verts_per_surf)\n",
    "                )\n",
    "\n",
    "            base_t_idx = np.where((ts_time >= -200) & (ts_time <= -100))[0]\n",
    "            m_base = np.mean(np.abs(all_layer_ts[:, base_t_idx]), axis=-1)\n",
    "            t_idx = np.where((ts_time >= 50) & (ts_time <= 150))[0]\n",
    "            signal_mag = np.mean(np.abs(all_layer_ts[:, t_idx]), axis=-1) - m_base\n",
    "\n",
    "                # Compute anatomical predictors\n",
    "            thickness = get_cortical_thickness(multilayer_mesh, n_layers)\n",
    "            gainmat_fname = os.path.join(out_dir, f'SPMgainmatrix_{data_base}_1.mat')\n",
    "            lf_rmse = get_lead_field_rmse(gainmat_fname, n_layers, verts_per_surf)\n",
    "            scalp_mesh_fname = os.path.join(mri_data_path, f'{sub}-synth', 'mri/T1scalp_2562.surf.gii')\n",
    "            orientations = get_orientation(scalp_mesh_fname, multilayer_mesh, ds_pial, verts_per_surf)\n",
    "            distances = get_dist_to_scalp(scalp_mesh_fname, ds_pial)\n",
    "\n",
    "                # Z-score each anatomical predictor (note: invert distance to scalp)\n",
    "            z_thickness = zscore(thickness)\n",
    "            z_lf_rmse = zscore(lf_rmse)\n",
    "            z_orient = zscore(orientations)\n",
    "            z_inv_dist = zscore(-distances)\n",
    "\n",
    "                # Composite anatomical score\n",
    "            anatomical_score = z_thickness + z_lf_rmse + z_orient + z_inv_dist\n",
    "\n",
    "            perc_thresh = 99\n",
    "\n",
    "                # Restrict to roi_idx - LH\n",
    "            roi_anat_score = anatomical_score[lh_roi_idx]\n",
    "            roi_signal = signal_mag[lh_roi_idx]\n",
    "\n",
    "                # Select top 1% signal vertices within ROI\n",
    "            signal_threshold = np.percentile(roi_signal, perc_thresh)\n",
    "            high_signal_mask = roi_signal >= signal_threshold\n",
    "            candidate_indices = lh_roi_idx[high_signal_mask]\n",
    "\n",
    "                # Among them, select the vertex with the best anatomical suitability\n",
    "            candidate_anat_scores = anatomical_score[candidate_indices]\n",
    "            best_idx_local = np.argmax(candidate_anat_scores)\n",
    "            lh_prior = candidate_indices[best_idx_local]\n",
    "\n",
    "                # Get source time series from middle surface layer\n",
    "            lh_prior_ts = all_layer_ts[lh_prior, :]\n",
    "\n",
    "            # Run sliding time window model comparison\n",
    "            [lh_Fs, wois] = sliding_window_model_comparison(\n",
    "                    lh_prior,\n",
    "                    nas,\n",
    "                    lpa,\n",
    "                    rpa,\n",
    "                    mri_fname,\n",
    "                    layer_fnames,\n",
    "                    base_fname,\n",
    "                    spm_instance=spm,\n",
    "                    viz=False,\n",
    "                    invert_kwargs={\n",
    "                        'patch_size': patch_size,\n",
    "                        'n_temp_modes': 1,\n",
    "                        'n_spatial_modes': 60,\n",
    "                        'win_size': 10,\n",
    "                        'win_overlap': True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "                # Restrict to roi_idx - RH\n",
    "            roi_anat_score = anatomical_score[rh_roi_idx]\n",
    "            roi_signal = signal_mag[rh_roi_idx]\n",
    "\n",
    "                # Select top 1% signal vertices within ROI\n",
    "            signal_threshold = np.percentile(roi_signal, perc_thresh)\n",
    "            high_signal_mask = roi_signal >= signal_threshold\n",
    "            candidate_indices = rh_roi_idx[high_signal_mask]\n",
    "\n",
    "                # Among them, select the vertex with the best anatomical suitability\n",
    "            candidate_anat_scores = anatomical_score[candidate_indices]\n",
    "            best_idx_local = np.argmax(candidate_anat_scores)\n",
    "            rh_prior = candidate_indices[best_idx_local]\n",
    "\n",
    "                # Get source time series from middle surface layer\n",
    "            rh_prior_ts = all_layer_ts[rh_prior, :]\n",
    "\n",
    "\n",
    "                # Run sliding time window model comparison\n",
    "            [rh_Fs, wois] = sliding_window_model_comparison(\n",
    "                    rh_prior,\n",
    "                    nas,\n",
    "                    lpa,\n",
    "                    rpa,\n",
    "                    mri_fname,\n",
    "                    layer_fnames,\n",
    "                    base_fname,\n",
    "                    spm_instance=spm,\n",
    "                    viz=False,\n",
    "                    invert_kwargs={\n",
    "                        'patch_size': patch_size,\n",
    "                        'n_temp_modes': 1,\n",
    "                        'n_spatial_modes': 60,\n",
    "                        'win_size': 10,\n",
    "                        'win_overlap': True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            np.savez(\n",
    "                    out_fname,\n",
    "                    subject=subject,\n",
    "                    session=session,\n",
    "                    lh_prior=lh_prior,\n",
    "                    rh_prior=rh_prior,\n",
    "                    lh_prior_ts=lh_prior_ts,\n",
    "                    rh_prior_ts=rh_prior_ts,\n",
    "                    ts_time=ts_time,\n",
    "                    lh_Fs=lh_Fs,\n",
    "                    rh_Fs=rh_Fs,\n",
    "                    wois=wois\n",
    "                )\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    spm = spm_standalone.initialize()\n",
    "    run(spm)\n",
    "    spm.terminate()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
